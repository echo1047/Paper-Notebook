# planning in stochastic environments with a learned model

## 4.1 Stochastic Model

在推理过程中，给定初始观测值$o_{\le t}$和动作序列$a_{t:t+K}$，我们可以递归展开上述模型，并从分布$c_{t+k+1} \sim \sigma_{t}^{k}$中抽样机会结果，来生成轨迹。

### 偶然结果 (chance outcome)

随机MuZero通过使用VQ-VAE方法的一种新变体来模拟机会结果。具体地，我们考虑一个大小为M的常量码本的VQ-VAE。码本中的每一项都是一个大小为M的固定独热向量。利用一个独热向量的固定码本，我们可以简化VQ-VAE的方程。在这种情况下，我们将嵌入$c^e_t$的编码器建模为一个分类变量，选择最接近的编码$c_t$相当于计算表达式$\text { one hot }(\arg \max _{i}(c_{t}^{e, i}))$。由此产生的编码器也可以视为观测的随机函数，该函数利用了Gumbel softmax重参数化技巧(Jang等人，2016)，向前通过时温度为零，向后通过时为直通估计器。在我们的模型中没有显式解码器，与之前的工作相反(Ozair et al.， 2021)，我们没有使用重构损失。相反，该网络以类似于MuZero的方式进行端到端训练。在下一节中，我们将更详细地解释培训过程。

>首先从类型上来讲，encoder和decoder指的是模型，embedding指的是tensor。
>encoder 编码器，将信息/数据进行编码，或进行特征提取（可以看做更复杂的编码）
>decoder 解码器，将特征解码为任务相关的输出。如词向量->词，feature map->检测框+类别序列，feature -> 生成图像。
>embedding 比较特殊，在不同任务和模型下会有具体的指代。一般来讲，我们会对简单处理后的数字化的数据叫embedding，如transformer前的token。但是在有的场景下，也会管模型（如encoder）输出的特征叫embedding。

>总结一下：encoder 用来提取、凝练（降维）特征embedding 指由某种能理解的模态提取得到的特征或数字化的张量decoder用于将特征向量转化为我们需要的结果。

### 模型训练

$$L^{\text {total }}=L^{\text {MuZero }}+L^{\text {chance }}$$

### (A) 蒙特卡罗树搜索

随机MuZero中采用蒙特卡罗树搜索，其中菱形节点表示机会节点，圆形节点表示决策节点。在选择过程中，对于机会节点采用先验分布$\sigma$抽样选择边,对于决策节点采用pUCT公式选择边。

### (B) 训练随机模型

例如给出一条长度为2的轨迹,包括观测$o_{\le t:t+2}$,动作$a_{t:t+2}$,价值目标$z_{t:t+2}$,策略目标${\pi}_{t:t+2}$,奖励$u_{t+1:t+K}$. 模型将推演两步

编码器$e$输入为$o_{\le t+k}$,确定性地生产偶然结果$c_{t+k}$. 模型的策略,价值和奖励结果分别向目标逼近

对未来的编码的分布$\sigma^{k}$进行训练，以预测编码器产生的编码。

### 随机搜索

在随机MuZero中引入机会节点和机会值，扩展了MCTS算法。在MCTS的随机实例化中，存在两种节点类型:decision和chance (Couetoux, 2013)。机会节点和决策节点沿着树的深度交错排列，因此每个决策节点的父节点都是一个机会节点。树的根节点总是一个决策节点。

在我们的方法中，每个机会节点对应一个潜在的后状态(4.1)，它通过查询随机模型进行扩展，其中父状态和一个动作作为输入，模型返回节点的价值和未来编码$Pr(c|as)$上的先验分布。在一个机会节点被扩展后，它的值被反向传播到树中。最后，当在选择阶段遍历节点时，通过对先验分布采样来选择一个编码。在Stochastic MuZero中，每个内部决策节点通过查询学习到的模型再次扩展，其中机会节点的父节点的状态和一个采样的编码c作为输入，模型返回一个奖励、一个价值和一个策略。与MuZero类似，新添加节点的值向上反向传播到树中，并使用pUCT(2)公式来选择一条边。随机MuZero使用的随机搜索如图1所示。


主要的创新点:
