{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda activate corl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section, we develop a conservative Q-algorithm, such that the expected value of a policy under the learned Q-function lower-bounds its true value. Lower-bounded Q-values prevent the over-estimation that is common in offline RL settings due to OOD $\\pi$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\pi$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝尔曼算子$\\bf{B}_{\\pi},\\bf{B}_{*}$定义为将一个价值函数向量转换为另一个价值函数向量的算子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{\\mathrm{Q}}^{\\mathrm{k}+1} \\leftarrow \\arg \\min _{\\mathrm{Q}} \\mathbb{E}_{\\mathbf{s}, \\mathbf{a}, \\mathbf{s}^{\\prime} \\sim \\mathcal{D}}\\left[\\left(\\left(\\mathrm{r}(\\mathbf{s}, \\mathbf{a})+\\gamma \\mathbb{E}_{\\mathbf{a}^{\\prime} \\sim \\hat{\\pi}^{\\mathrm{k}}\\left(\\mathbf{a}^{\\prime} \\mid \\mathbf{s}^{\\prime}\\right)}\\left[\\hat{\\mathrm{Q}}^{\\mathrm{k}}\\left(\\mathbf{s}^{\\prime}, \\mathbf{a}^{\\prime}\\right)\\right]\\right)-\\mathrm{Q}(\\mathbf{s}, \\mathbf{a})\\right)^{2}\\right]$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
